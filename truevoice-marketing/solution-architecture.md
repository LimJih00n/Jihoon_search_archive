# ğŸ—ï¸ TrueVoice ê¸°ìˆ  ì•„í‚¤í…ì²˜ ì„¤ê³„

## 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### ğŸ¯ í•µì‹¬ ì„¤ê³„ ì›ì¹™

```yaml
Design Principles:
1. Scalability First: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
2. Real-time Processing: ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°
3. AI-Native: ML íŒŒì´í”„ë¼ì¸ ë‚´ì¥
4. Privacy by Design: ê°œì¸ì •ë³´ ë³´í˜¸ ìš°ì„ 
5. Multi-tenancy: ì—”í„°í”„ë¼ì´ì¦ˆ ì§€ì›
```

### ğŸ›ï¸ High-Level Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        A[Perigon API]
        B[Social Media APIs]
        C[Web Crawler]
        D[Community Platforms]
    end
    
    subgraph "Data Pipeline"
        E[Ingestion Layer]
        F[Stream Processing]
        G[Data Lake]
        H[Feature Store]
    end
    
    subgraph "AI/ML Layer"
        I[NLP Engine]
        J[Credibility Scorer]
        K[Prediction Model]
        L[Meme Analyzer]
    end
    
    subgraph "Application Layer"
        M[API Gateway]
        N[Business Logic]
        O[Cache Layer]
        P[Notification Service]
    end
    
    subgraph "Frontend"
        Q[Web Dashboard]
        R[Mobile App]
        S[Chrome Extension]
    end
    
    A --> E
    B --> E
    C --> E
    D --> E
    E --> F
    F --> G
    F --> H
    H --> I
    H --> J
    H --> K
    H --> L
    I --> N
    J --> N
    K --> N
    L --> N
    N --> M
    M --> Q
    M --> R
    M --> S
```

## 2. ë°ì´í„° ìˆ˜ì§‘ ë ˆì´ì–´

### ğŸ“Š Multi-Source Data Collection

#### 2.1 Perigon API Integration
```python
class PerigonConnector:
    """
    Perigon API í†µí•© ëª¨ë“ˆ
    - 70,000+ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ì†ŒìŠ¤
    - ì‹¤ì‹œê°„ ì›¹ ì½˜í…ì¸  ìˆ˜ì§‘
    - ë‚´ì¥ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„
    """
    
    def __init__(self, api_key: str):
        self.base_url = "https://api.perigon.com/v1"
        self.headers = {"x-api-key": api_key}
        self.rate_limiter = RateLimiter(requests_per_minute=100)
    
    async def fetch_articles(self, query: str, filters: dict) -> List[Article]:
        """
        ì‹¤ì‹œê°„ ê¸°ì‚¬ ìˆ˜ì§‘
        - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        - ì§€ì—­/ì–¸ì–´ í•„í„°ë§
        - ì‹œê°„ ë²”ìœ„ ì„¤ì •
        """
        params = {
            "q": query,
            "language": filters.get("language", "ko"),
            "from": filters.get("from_date"),
            "to": filters.get("to_date"),
            "size": 100
        }
        
        async with self.rate_limiter:
            response = await self.http_client.get(
                f"{self.base_url}/search",
                headers=self.headers,
                params=params
            )
            
        return self._parse_articles(response.json())
```

#### 2.2 Social Media APIs
```python
class SocialMediaCollector:
    """
    ì†Œì…œë¯¸ë””ì–´ í†µí•© ìˆ˜ì§‘ê¸°
    """
    
    platforms = {
        "twitter": TwitterAPI(),      # X(Twitter) API v2
        "instagram": InstagramAPI(),   # Instagram Basic Display API
        "youtube": YouTubeAPI(),       # YouTube Data API v3
        "tiktok": TikTokAPI(),        # TikTok API
        "reddit": RedditAPI()          # Reddit API
    }
    
    async def collect_mentions(self, brand: str, timeframe: int) -> dict:
        """
        ë¸Œëœë“œ ë©˜ì…˜ ìˆ˜ì§‘
        - ì‹¤ì‹œê°„ ë©˜ì…˜ ì¶”ì 
        - ëŒ“ê¸€/ë¦¬í”Œë¼ì´ ìˆ˜ì§‘
        - ë°ˆ/ì´ë¯¸ì§€ í¬í•¨
        """
        results = {}
        
        tasks = []
        for platform_name, api in self.platforms.items():
            task = api.search_mentions(
                query=brand,
                hours_back=timeframe,
                include_replies=True,
                include_media=True
            )
            tasks.append(task)
        
        platform_data = await asyncio.gather(*tasks)
        
        for platform_name, data in zip(self.platforms.keys(), platform_data):
            results[platform_name] = data
            
        return results
```

#### 2.3 Korean Community Crawler
```python
class KoreanCommunityCrawler:
    """
    í•œêµ­ ì»¤ë®¤ë‹ˆí‹° íŠ¹í™” í¬ë¡¤ëŸ¬
    """
    
    target_sites = {
        "dcinside": DCInsideCrawler(),
        "fmkorea": FMKoreaCrawler(),
        "clien": ClienCrawler(),
        "ruliweb": RuliwebCrawler(),
        "naver_cafe": NaverCafeCrawler(),
        "blind": BlindCrawler()
    }
    
    async def crawl_posts(self, keywords: List[str]) -> List[Post]:
        """
        ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ í¬ë¡¤ë§
        - ì‹¤ì‹œê°„ ì¸ê¸°ê¸€
        - ëŒ“ê¸€ ë°˜ì‘
        - ì¶”ì²œ/ë¹„ì¶”ì²œ ìˆ˜
        """
        all_posts = []
        
        for site_name, crawler in self.target_sites.items():
            try:
                posts = await crawler.search_posts(
                    keywords=keywords,
                    sort_by="popular",
                    limit=100
                )
                all_posts.extend(posts)
            except Exception as e:
                logger.error(f"Error crawling {site_name}: {e}")
                
        return all_posts
```

### ğŸ“ˆ Data Volume Estimation

```yaml
ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ëŸ‰:
- ë‰´ìŠ¤ ê¸°ì‚¬: 50,000ê±´
- ì†Œì…œë¯¸ë””ì–´ í¬ìŠ¤íŠ¸: 500,000ê±´
- ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€: 100,000ê±´
- ëŒ“ê¸€/ë¦¬í”Œë¼ì´: 2,000,000ê±´
- ì´ë¯¸ì§€/ë°ˆ: 50,000ê±´

ì´ ì¼ì¼ ì²˜ë¦¬ëŸ‰: ~3TB
ì›”ê°„ ìŠ¤í† ë¦¬ì§€: ~90TB
```

## 3. AI/ML íŒŒì´í”„ë¼ì¸

### ğŸ¤– 3.1 NLP ì—”ì§„

```python
class KoreanNLPEngine:
    """
    í•œêµ­ì–´ íŠ¹í™” ìì—°ì–´ ì²˜ë¦¬ ì—”ì§„
    """
    
    def __init__(self):
        # KoELECTRA for Korean sentiment analysis
        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(
            "tunib/electra-ko-base-sentiment"
        )
        
        # GPT-4 for intent analysis
        self.intent_analyzer = OpenAI(
            model="gpt-4",
            temperature=0.3
        )
        
        # Custom slang/meme dictionary
        self.slang_dict = self._load_slang_dictionary()
        self.meme_patterns = self._load_meme_patterns()
    
    async def analyze_text(self, text: str) -> dict:
        """
        í…ìŠ¤íŠ¸ ì¢…í•© ë¶„ì„
        """
        # 1. ì „ì²˜ë¦¬ (ì€ì–´/ì‹ ì¡°ì–´ ì •ê·œí™”)
        normalized_text = self._normalize_slang(text)
        
        # 2. ê°ì„± ë¶„ì„
        sentiment = await self._analyze_sentiment(normalized_text)
        
        # 3. ì˜ë„ ë¶„ì„
        intent = await self._analyze_intent(normalized_text)
        
        # 4. ë°ˆ/íŠ¸ë Œë“œ ê°ì§€
        memes = self._detect_memes(text)
        
        # 5. êµ¬ë§¤ ì˜ë„ ì ìˆ˜
        purchase_intent = self._calculate_purchase_intent(
            sentiment, intent, text
        )
        
        return {
            "sentiment": sentiment,
            "intent": intent,
            "memes": memes,
            "purchase_intent": purchase_intent,
            "original_text": text,
            "normalized_text": normalized_text
        }
```

### ğŸ¯ 3.2 ì‹ ë¢°ë„ í‰ê°€ ëª¨ë¸

```python
class CredibilityScorer:
    """
    ëŒ“ê¸€/í¬ìŠ¤íŠ¸ ì‹ ë¢°ë„ í‰ê°€ ëª¨ë¸
    ì§„ì§œ ì˜ê²¬ vs ë´‡/ì–´ë·°ì§• êµ¬ë¶„
    """
    
    def __init__(self):
        self.bot_detector = BotDetectionModel()
        self.authenticity_scorer = AuthenticityModel()
        self.user_profiler = UserProfileAnalyzer()
    
    def calculate_credibility(self, post: dict) -> float:
        """
        ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-100)
        """
        score = 100  # ê¸°ë³¸ ì ìˆ˜
        
        # ë´‡ ê°ì§€ (-50ì )
        if self.bot_detector.is_bot(post):
            score -= 50
            
        # ê³„ì • ë‚˜ì´ (+10ì  if > 1ë…„)
        account_age = self._get_account_age(post["user"])
        if account_age > 365:
            score += 10
            
        # íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ ì²´í¬
        follower_ratio = post["followers"] / max(post["following"], 1)
        if follower_ratio < 0.1:  # íŒ”ë¡œì›Œê°€ ë„ˆë¬´ ì ìŒ
            score -= 20
            
        # ê·¹ë‹¨ì  ì–¸ì–´ ì‚¬ìš© (-20ì )
        if self._contains_extreme_language(post["text"]):
            score -= 20
            
        # êµ¬ì²´ì  ê²½í—˜ ì–¸ê¸‰ (+30ì )
        if self._mentions_personal_experience(post["text"]):
            score += 30
            
        # ì´ë¯¸ì§€/ì˜ìƒ í¬í•¨ (+15ì )
        if post.get("media"):
            score += 15
            
        # ê³¼ë„í•œ í•´ì‹œíƒœê·¸ (-15ì )
        hashtag_count = len(post.get("hashtags", []))
        if hashtag_count > 10:
            score -= 15
            
        # ë°˜ë³µì  íŒ¨í„´ ê°ì§€ (-30ì )
        if self._detect_repetitive_pattern(post["user"]):
            score -= 30
            
        return max(0, min(100, score))
```

### ğŸ“Š 3.3 ì‹¤ì œ ë°˜ì‘ ì˜ˆì¸¡ ëª¨ë¸

```python
class RealWorldPredictor:
    """
    ì˜¨ë¼ì¸ ë°˜ì‘ â†’ ì‹¤ì œ í–‰ë™ ì˜ˆì¸¡ ëª¨ë¸
    """
    
    def __init__(self):
        # ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµëœ ë³€í™˜ ëª¨ë¸
        self.conversion_model = self._load_trained_model()
        
        # í”Œë«í¼ë³„ ê°€ì¤‘ì¹˜
        self.platform_weights = {
            "news_comment": 0.3,
            "twitter": 0.5,
            "instagram": 0.7,
            "community": 0.8,
            "review_site": 0.9
        }
        
        # ì¸êµ¬í†µê³„ ê°€ì¤‘ì¹˜
        self.demographic_weights = self._load_demographic_weights()
    
    def predict_real_reaction(self, online_data: dict) -> dict:
        """
        ì‹¤ì œ ì„¸ìƒ ë°˜ì‘ ì˜ˆì¸¡
        """
        # 1. í”Œë«í¼ë³„ ë°ì´í„° ì§‘ê³„
        platform_sentiments = {}
        for platform, posts in online_data.items():
            # ì‹ ë¢°ë„ ë†’ì€ í¬ìŠ¤íŠ¸ë§Œ í•„í„°ë§
            credible_posts = [
                p for p in posts 
                if p["credibility_score"] > 60
            ]
            
            # ê°€ì¤‘ í‰ê·  ê°ì„± ê³„ì‚°
            weighted_sentiment = self._calculate_weighted_sentiment(
                credible_posts,
                self.platform_weights.get(platform, 0.5)
            )
            
            platform_sentiments[platform] = weighted_sentiment
        
        # 2. ì¸êµ¬í†µê³„ ë³´ì •
        adjusted_sentiment = self._apply_demographic_adjustment(
            platform_sentiments,
            self.demographic_weights
        )
        
        # 3. ì‹¤ì œ í–‰ë™ ì˜ˆì¸¡
        prediction = self.conversion_model.predict({
            "online_sentiment": adjusted_sentiment,
            "volume": len(online_data),
            "velocity": self._calculate_velocity(online_data),
            "diversity": self._calculate_diversity(online_data)
        })
        
        return {
            "predicted_purchase_intent": prediction["purchase_intent"],
            "predicted_brand_sentiment": prediction["brand_sentiment"],
            "predicted_viral_potential": prediction["viral_potential"],
            "confidence_score": prediction["confidence"],
            "risk_factors": self._identify_risks(online_data),
            "opportunities": self._identify_opportunities(online_data)
        }
```

### ğŸ­ 3.4 ë°ˆ/ë°”ì´ëŸ´ ë¶„ì„ê¸°

```python
class MemeAnalyzer:
    """
    ë°ˆ íŠ¸ë Œë“œ ë° ë°”ì´ëŸ´ ì ì¬ë ¥ ë¶„ì„
    """
    
    def __init__(self):
        # ì´ë¯¸ì§€ ì¸ì‹ ëª¨ë¸ (ë°ˆ í…œí”Œë¦¿ ê°ì§€)
        self.vision_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # ë°ˆ ë°ì´í„°ë² ì´ìŠ¤
        self.meme_db = MemeDatabase()
        
        # ë°”ì´ëŸ´ ì˜ˆì¸¡ ëª¨ë¸
        self.viral_predictor = ViralPredictionModel()
    
    async def analyze_meme(self, content: dict) -> dict:
        """
        ë°ˆ ì½˜í…ì¸  ë¶„ì„
        """
        analysis = {}
        
        # 1. ë°ˆ í…œí”Œë¦¿ ì¸ì‹
        if content.get("image"):
            template = await self._identify_meme_template(content["image"])
            analysis["template"] = template
            analysis["template_popularity"] = self.meme_db.get_popularity(template)
        
        # 2. í…ìŠ¤íŠ¸ ë°ˆ íŒ¨í„´ ê°ì§€
        if content.get("text"):
            text_memes = self._detect_text_memes(content["text"])
            analysis["text_memes"] = text_memes
        
        # 3. ë°”ì´ëŸ´ ì ì¬ë ¥ ê³„ì‚°
        viral_score = self.viral_predictor.predict({
            "engagement_rate": content.get("engagement_rate", 0),
            "share_velocity": content.get("share_velocity", 0),
            "platform": content.get("platform"),
            "meme_freshness": self._calculate_freshness(content),
            "cultural_relevance": self._assess_cultural_relevance(content)
        })
        
        analysis["viral_potential"] = viral_score
        
        # 4. ìˆ˜ëª… ì˜ˆì¸¡
        lifespan = self._predict_meme_lifespan(content)
        analysis["predicted_lifespan_days"] = lifespan
        
        # 5. ë¸Œëœë“œ ì í•©ì„± í‰ê°€
        brand_fit = self._assess_brand_fit(content)
        analysis["brand_safety_score"] = brand_fit["safety"]
        analysis["brand_alignment_score"] = brand_fit["alignment"]
        
        return analysis
```

## 4. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥

### ğŸ’¾ 4.1 Data Pipeline Architecture

```python
class DataPipeline:
    """
    ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self):
        # Apache Kafka for streaming
        self.kafka_producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        # Apache Flink for stream processing
        self.flink_env = StreamExecutionEnvironment.get_execution_environment()
        
        # PostgreSQL for structured data
        self.postgres = PostgresClient()
        
        # MongoDB for unstructured data
        self.mongodb = MongoClient()
        
        # Redis for caching
        self.redis = RedisClient()
        
        # S3 for data lake
        self.s3 = S3Client()
    
    async def process_incoming_data(self, raw_data: dict):
        """
        ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
        """
        # 1. Kafkaë¡œ ìŠ¤íŠ¸ë¦¬ë°
        self.kafka_producer.send('raw-data-topic', raw_data)
        
        # 2. ì‹¤ì‹œê°„ ì²˜ë¦¬ (Flink)
        processed_data = await self._stream_process(raw_data)
        
        # 3. ë°ì´í„° ì €ì¥
        await self._store_data(processed_data)
        
        # 4. ìºì‹œ ì—…ë°ì´íŠ¸
        await self._update_cache(processed_data)
        
        # 5. ì‹¤ì‹œê°„ ì•Œë¦¼ íŠ¸ë¦¬ê±°
        await self._trigger_alerts(processed_data)
```

### ğŸ—„ï¸ 4.2 Database Schema

```sql
-- PostgreSQL Schema

-- ë¸Œëœë“œ í…Œì´ë¸”
CREATE TABLE brands (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    industry VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ìº í˜ì¸ í…Œì´ë¸”
CREATE TABLE campaigns (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    brand_id UUID REFERENCES brands(id),
    name VARCHAR(255) NOT NULL,
    start_date DATE,
    end_date DATE,
    budget DECIMAL(12, 2),
    target_keywords TEXT[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ë©˜ì…˜ í…Œì´ë¸”
CREATE TABLE mentions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    campaign_id UUID REFERENCES campaigns(id),
    platform VARCHAR(50),
    author_id VARCHAR(255),
    content TEXT,
    sentiment_score FLOAT,
    credibility_score FLOAT,
    engagement_count INTEGER,
    created_at TIMESTAMP,
    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸”
CREATE TABLE predictions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    campaign_id UUID REFERENCES campaigns(id),
    prediction_type VARCHAR(50), -- 'purchase_intent', 'viral_potential', etc.
    predicted_value FLOAT,
    confidence_score FLOAT,
    actual_value FLOAT, -- ì‹¤ì œ ê²°ê³¼ (ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸)
    predicted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ë°ˆ ë¶„ì„ í…Œì´ë¸”
CREATE TABLE meme_analysis (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    mention_id UUID REFERENCES mentions(id),
    meme_template VARCHAR(255),
    viral_score FLOAT,
    predicted_lifespan_days INTEGER,
    brand_safety_score FLOAT,
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_mentions_campaign ON mentions(campaign_id);
CREATE INDEX idx_mentions_platform ON mentions(platform);
CREATE INDEX idx_mentions_created ON mentions(created_at);
CREATE INDEX idx_predictions_campaign ON predictions(campaign_id);
```

### ğŸ“Š 4.3 Feature Store

```python
class FeatureStore:
    """
    ML ëª¨ë¸ì„ ìœ„í•œ Feature Store
    """
    
    def __init__(self):
        self.store = FeatureStoreClient()
    
    def create_features(self):
        """
        í”¼ì²˜ ì •ì˜ ë° ìƒì„±
        """
        # ë¸Œëœë“œ í”¼ì²˜
        brand_features = self.store.create_feature_group(
            name="brand_features",
            features=[
                Feature("brand_id", ValueType.STRING),
                Feature("industry", ValueType.STRING),
                Feature("avg_sentiment_30d", ValueType.FLOAT),
                Feature("mention_volume_30d", ValueType.INT64),
                Feature("top_keywords", ValueType.STRING_LIST)
            ]
        )
        
        # ìº í˜ì¸ í”¼ì²˜
        campaign_features = self.store.create_feature_group(
            name="campaign_features",
            features=[
                Feature("campaign_id", ValueType.STRING),
                Feature("days_active", ValueType.INT64),
                Feature("daily_mention_avg", ValueType.FLOAT),
                Feature("sentiment_trend", ValueType.FLOAT),
                Feature("viral_score_avg", ValueType.FLOAT)
            ]
        )
        
        # ì‹¤ì‹œê°„ í”¼ì²˜
        realtime_features = self.store.create_feature_group(
            name="realtime_features",
            features=[
                Feature("current_velocity", ValueType.FLOAT),
                Feature("spike_detected", ValueType.BOOL),
                Feature("trending_topics", ValueType.STRING_LIST),
                Feature("risk_score", ValueType.FLOAT)
            ]
        )
        
        return [brand_features, campaign_features, realtime_features]
```

## 5. API ë° ì„œë¹„ìŠ¤ ë ˆì´ì–´

### ğŸš€ 5.1 RESTful API Design

```python
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel

app = FastAPI(title="TrueVoice API", version="1.0.0")

# API Models
class CampaignRequest(BaseModel):
    brand_name: str
    keywords: List[str]
    platforms: List[str]
    date_range: dict

class PredictionResponse(BaseModel):
    campaign_id: str
    predictions: dict
    confidence: float
    insights: List[str]
    recommendations: List[str]

# API Endpoints
@app.post("/api/v1/campaigns", response_model=dict)
async def create_campaign(request: CampaignRequest):
    """
    ìƒˆ ìº í˜ì¸ ìƒì„± ë° ëª¨ë‹ˆí„°ë§ ì‹œì‘
    """
    campaign = await campaign_service.create(
        brand=request.brand_name,
        keywords=request.keywords,
        platforms=request.platforms
    )
    
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
    background_tasks.add_task(
        data_collector.start_collection,
        campaign.id
    )
    
    return {
        "campaign_id": campaign.id,
        "status": "active",
        "monitoring_started": True
    }

@app.get("/api/v1/predictions/{campaign_id}")
async def get_predictions(campaign_id: str) -> PredictionResponse:
    """
    ìº í˜ì¸ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ
    """
    predictions = await prediction_service.get_latest(campaign_id)
    
    return PredictionResponse(
        campaign_id=campaign_id,
        predictions=predictions,
        confidence=predictions["confidence_score"],
        insights=insight_generator.generate(predictions),
        recommendations=recommendation_engine.suggest(predictions)
    )

@app.get("/api/v1/realtime/{campaign_id}")
async def get_realtime_data(campaign_id: str):
    """
    ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ (WebSocket)
    """
    return await websocket_manager.stream_realtime_data(campaign_id)

@app.post("/api/v1/analyze/meme")
async def analyze_meme(content: dict):
    """
    ë°ˆ ì½˜í…ì¸  ë¶„ì„
    """
    analysis = await meme_analyzer.analyze(content)
    return analysis
```

### ğŸ”Œ 5.2 WebSocket for Real-time Updates

```python
class WebSocketManager:
    """
    ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ WebSocket ê´€ë¦¬
    """
    
    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, campaign_id: str):
        await websocket.accept()
        if campaign_id not in self.active_connections:
            self.active_connections[campaign_id] = []
        self.active_connections[campaign_id].append(websocket)
    
    async def broadcast_update(self, campaign_id: str, data: dict):
        """
        íŠ¹ì • ìº í˜ì¸ êµ¬ë…ìë“¤ì—ê²Œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì „ì†¡
        """
        if campaign_id in self.active_connections:
            for connection in self.active_connections[campaign_id]:
                try:
                    await connection.send_json({
                        "type": "update",
                        "timestamp": datetime.now().isoformat(),
                        "data": data
                    })
                except:
                    # ì—°ê²° ëŠê¹€ ì²˜ë¦¬
                    self.active_connections[campaign_id].remove(connection)

@app.websocket("/ws/{campaign_id}")
async def websocket_endpoint(websocket: WebSocket, campaign_id: str):
    await manager.connect(websocket, campaign_id)
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
            data = await websocket.receive_text()
            
            # ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
            realtime_data = await get_realtime_metrics(campaign_id)
            await websocket.send_json(realtime_data)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket, campaign_id)
```

## 6. ë³´ì•ˆ ë° ì„±ëŠ¥ ìµœì í™”

### ğŸ” 6.1 Security Implementation

```python
class SecurityLayer:
    """
    ë³´ì•ˆ ë ˆì´ì–´ êµ¬í˜„
    """
    
    def __init__(self):
        self.encryptor = Fernet(settings.ENCRYPTION_KEY)
        self.rate_limiter = RateLimiter()
        self.auth_manager = AuthenticationManager()
    
    # API ì¸ì¦
    async def verify_api_key(self, api_key: str) -> bool:
        """
        API í‚¤ ê²€ì¦
        """
        hashed_key = hashlib.sha256(api_key.encode()).hexdigest()
        return await self.auth_manager.verify_key(hashed_key)
    
    # ë°ì´í„° ì•”í˜¸í™”
    def encrypt_sensitive_data(self, data: str) -> str:
        """
        ë¯¼ê° ë°ì´í„° ì•”í˜¸í™”
        """
        return self.encryptor.encrypt(data.encode()).decode()
    
    # Rate Limiting
    @app.middleware("http")
    async def rate_limit_middleware(request: Request, call_next):
        client_ip = request.client.host
        
        if not rate_limiter.is_allowed(client_ip):
            raise HTTPException(
                status_code=429,
                detail="Too many requests"
            )
        
        response = await call_next(request)
        return response
    
    # Data Privacy
    def anonymize_user_data(self, data: dict) -> dict:
        """
        ê°œì¸ì •ë³´ ìµëª…í™”
        """
        anonymized = data.copy()
        
        # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
        if "email" in anonymized:
            anonymized["email"] = self._mask_email(data["email"])
        
        # ì´ë¦„ í•´ì‹±
        if "name" in anonymized:
            anonymized["name"] = hashlib.md5(
                data["name"].encode()
            ).hexdigest()[:8]
        
        # IP ì£¼ì†Œ ì œê±°
        if "ip_address" in anonymized:
            del anonymized["ip_address"]
        
        return anonymized
```

### âš¡ 6.2 Performance Optimization

```python
class PerformanceOptimizer:
    """
    ì„±ëŠ¥ ìµœì í™” ì „ëµ
    """
    
    def __init__(self):
        # Redis ìºì‹±
        self.cache = RedisCache(
            host='localhost',
            port=6379,
            ttl=300  # 5ë¶„ ìºì‹œ
        )
        
        # Connection pooling
        self.db_pool = create_pool(
            min_size=10,
            max_size=100
        )
        
        # Async processing
        self.executor = ThreadPoolExecutor(max_workers=20)
    
    # ìºì‹± ì „ëµ
    @cache_result(ttl=300)
    async def get_cached_predictions(self, campaign_id: str):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ìºì‹±
        """
        cache_key = f"predictions:{campaign_id}"
        
        # ìºì‹œ í™•ì¸
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ ê³„ì‚°
        predictions = await self._calculate_predictions(campaign_id)
        
        # ìºì‹œ ì €ì¥
        await self.cache.set(cache_key, predictions)
        
        return predictions
    
    # ë°°ì¹˜ ì²˜ë¦¬
    async def batch_process_mentions(self, mentions: List[dict]):
        """
        ëŒ€ëŸ‰ ë©˜ì…˜ ë°°ì¹˜ ì²˜ë¦¬
        """
        batch_size = 1000
        batches = [
            mentions[i:i + batch_size]
            for i in range(0, len(mentions), batch_size)
        ]
        
        tasks = []
        for batch in batches:
            task = self._process_batch(batch)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return list(itertools.chain(*results))
    
    # ì¿¼ë¦¬ ìµœì í™”
    def optimize_database_queries(self):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”
        """
        # ì¸ë±ìŠ¤ ìƒì„±
        create_indexes = [
            "CREATE INDEX CONCURRENTLY idx_mentions_timestamp ON mentions(created_at DESC)",
            "CREATE INDEX idx_mentions_sentiment ON mentions(sentiment_score)",
            "CREATE INDEX idx_campaigns_brand ON campaigns(brand_id)"
        ]
        
        # íŒŒí‹°ì…”ë‹
        partition_tables = """
        CREATE TABLE mentions_2025_01 PARTITION OF mentions
        FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
        """
        
        # Materialized Views
        create_materialized_view = """
        CREATE MATERIALIZED VIEW campaign_daily_stats AS
        SELECT 
            campaign_id,
            DATE(created_at) as date,
            COUNT(*) as mention_count,
            AVG(sentiment_score) as avg_sentiment,
            AVG(credibility_score) as avg_credibility
        FROM mentions
        GROUP BY campaign_id, DATE(created_at)
        WITH DATA;
        """
```

## 7. ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜

### ğŸ“Š 7.1 Monitoring Stack

```yaml
monitoring_stack:
  metrics:
    - Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
    - Grafana (ì‹œê°í™”)
    
  logging:
    - ELK Stack (Elasticsearch, Logstash, Kibana)
    - Structured logging with correlation IDs
    
  tracing:
    - Jaeger (ë¶„ì‚° íŠ¸ë ˆì´ì‹±)
    - OpenTelemetry (í‘œì¤€í™”ëœ ê´€ì¸¡ì„±)
    
  alerting:
    - PagerDuty (ì¸ì‹œë˜íŠ¸ ê´€ë¦¬)
    - Slack notifications
    
  health_checks:
    - /health (ì‹œìŠ¤í…œ ìƒíƒœ)
    - /ready (ì¤€ë¹„ ìƒíƒœ)
    - /metrics (Prometheus ë©”íŠ¸ë¦­)
```

### ğŸš¨ 7.2 Alert Rules

```python
alert_rules = {
    "high_error_rate": {
        "condition": "error_rate > 1%",
        "severity": "critical",
        "action": "page_oncall"
    },
    "api_latency": {
        "condition": "p99_latency > 1000ms",
        "severity": "warning",
        "action": "slack_notification"
    },
    "data_pipeline_lag": {
        "condition": "pipeline_lag > 5_minutes",
        "severity": "warning",
        "action": "email_team"
    },
    "prediction_accuracy_drop": {
        "condition": "accuracy < 70%",
        "severity": "critical",
        "action": "page_ml_team"
    }
}
```

## 8. í™•ì¥ì„± ë° ë¯¸ë˜ ê³„íš

### ğŸ“ˆ 8.1 Scaling Strategy

```yaml
horizontal_scaling:
  - Kubernetes for container orchestration
  - Auto-scaling based on CPU/Memory/Request rate
  - Multi-region deployment for global coverage
  
vertical_scaling:
  - GPU instances for ML workloads
  - High-memory instances for caching
  - SSD storage for hot data
  
data_scaling:
  - Data partitioning by date/campaign
  - Read replicas for query distribution
  - CDN for static content
```

### ğŸ”® 8.2 Future Enhancements

```yaml
planned_features:
  v2.0:
    - Multi-language support (EN, JP, CN)
    - Video content analysis
    - Voice of Customer (VoC) integration
    
  v3.0:
    - Predictive campaign optimization
    - Automated A/B testing
    - Real-time bidding integration
    
  v4.0:
    - AR/VR content analysis
    - Metaverse monitoring
    - Web3 social monitoring
```

## 9. ë¹„ìš© ì¶”ì •

### ğŸ’° Infrastructure Cost Estimation

```yaml
ì›”ê°„ ì¸í”„ë¼ ë¹„ìš© (AWS ê¸°ì¤€):
  compute:
    - EC2 (4x m5.xlarge): $600
    - EKS cluster: $150
    - Lambda functions: $100
    
  storage:
    - RDS PostgreSQL: $300
    - S3 (100TB): $2,300
    - ElastiCache Redis: $200
    
  ml_services:
    - SageMaker: $500
    - GPU instances: $800
    
  data_transfer:
    - CloudFront CDN: $200
    - Data transfer: $300
    
  third_party_apis:
    - Perigon API: $500
    - OpenAI GPT-4: $300
    - Social Media APIs: $200
    
  monitoring:
    - CloudWatch: $100
    - DataDog: $200
    
ì´ ì›”ê°„ ë¹„ìš©: ~$6,550
ì—°ê°„ ë¹„ìš©: ~$78,600
```

## 10. ê²°ë¡ 

TrueVoiceì˜ ê¸°ìˆ  ì•„í‚¤í…ì²˜ëŠ” í™•ì¥ ê°€ëŠ¥í•˜ê³  ì•ˆì „í•˜ë©° ê³ ì„±ëŠ¥ì˜ ì‹œìŠ¤í…œìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 

### âœ… í•µì‹¬ ê¸°ìˆ ì  ì°¨ë³„í™”
1. **Multi-source data fusion**: Perigon API + ì†Œì…œë¯¸ë””ì–´ + ì»¤ë®¤ë‹ˆí‹° í†µí•©
2. **Advanced AI pipeline**: ì‹ ë¢°ë„ í•„í„°ë§ + ì˜ˆì¸¡ ëª¨ë¸
3. **Real-time processing**: ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜ë¡œ ì¦‰ê°ì  ì¸ì‚¬ì´íŠ¸
4. **Korean-optimized**: í•œêµ­ì–´ NLP ë° ë¬¸í™”ì  ë§¥ë½ ì´í•´
5. **Enterprise-ready**: ë³´ì•ˆ, í™•ì¥ì„±, ì‹ ë¢°ì„± ë³´ì¥

ì´ ì•„í‚¤í…ì²˜ëŠ” MVPë¶€í„° ì—”í„°í”„ë¼ì´ì¦ˆ ê·œëª¨ê¹Œì§€ ì„±ì¥ ê°€ëŠ¥í•œ ê²¬ê³ í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.

---

*Technical Architecture v1.0*
*Last Updated: 2025ë…„ 1ì›”*